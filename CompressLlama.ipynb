{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_default_dtype(torch.float16)\n",
    "sparse12_model_path = 'checkpoints/sparse_checkpoints/sparse_llama12_alpaca_sd.pth' # SparseGPT model with 1:2 sparsity pattern applied to 7B llama with alpaca as a fine-tuning dataset.\n",
    "device = torch.device('cuda:0')\n",
    "model = torch.load(sparse12_model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_packing(mask):\n",
    "    flat_mask = mask.to(torch.uint8).flatten()\n",
    "    reshaped_mask = flat_mask.view(-1, 8)\n",
    "    packed_mask = torch.zeros(reshaped_mask.shape[0], dtype=torch.uint8, device=mask.device)\n",
    "    for i in range(8):\n",
    "        packed_mask += reshaped_mask[:, i] << i\n",
    "    packed_mask = packed_mask.view(mask.shape[0], mask.shape[1]//8)\n",
    "    return packed_mask\n",
    "\n",
    "def compress_weights(param):\n",
    "    W = param.clone().detach()\n",
    "    sparsity_mask = (W != 0)\n",
    "    V = W.masked_select(sparsity_mask).view(W.shape[0], -1)\n",
    "    assert V.shape[1]==W.shape[1]//2, \"Incorrect sparsity pattern\"\n",
    "    return V.to(param.dtype), bit_packing(sparsity_mask)\n",
    "\n",
    "cur_model = model\n",
    "new_state_dict = {}\n",
    "old_size, new_size = 0, 0\n",
    "for name, p in cur_model.named_parameters():\n",
    "    if any(x in name for x in [\"wq\", \"wk\", \"wv\", \"wo\", \"w1\", \"w2\", \"w3\", \"query_w\", \"key_w\", \"value_w\", \"attn_w\"]) and len(p.shape) > 1:\n",
    "        V, sparsity_mask = compress_weights(p)\n",
    "        new_state_dict[name] = V\n",
    "        new_name_ls = name.split('.')\n",
    "        new_name_ls[-2] = new_name_ls[-2]+'_mask'\n",
    "        new_name = '.'.join(new_name_ls[:-1])\n",
    "        new_state_dict[new_name] = sparsity_mask\n",
    "    else:\n",
    "        new_state_dict[name] = p\n",
    "\n",
    "# compute the size of a state_dict\n",
    "old_size = sum(p.numel() for p in cur_model.parameters())\n",
    "new_size = sum(p.numel() for p in new_state_dict.values())\n",
    "print('Compressed model is {:.2f}x of the original model'.format(new_size/old_size*100))\n",
    "torch.save(new_state_dict, 'checkpoints/sparse_checkpoints/compressed_llama_alpaca.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the time of inference, use the following two functions to decompress the weights of each layer independently.\n",
    "def bit_unpacking(packed_mask):\n",
    "    rows, cols = packed_mask.shape\n",
    "    packed_mask = packed_mask.view(-1)\n",
    "    unpacked_mask = torch.zeros(packed_mask.shape[0]*8, dtype=torch.uint8, device=packed_mask.device)\n",
    "    for i in range(8):\n",
    "        unpacked_mask[i::8] = (packed_mask >> i).bitwise_and_(1)\n",
    "    unpacked_mask = unpacked_mask.view(rows, cols*8)\n",
    "    return unpacked_mask\n",
    "\n",
    "def decompress_weights(V, sparsity_mask, x):\n",
    "    sparsity_mask = bit_unpacking(sparsity_mask)\n",
    "    rows, cols = sparsity_mask.shape\n",
    "    x = x.view(-1, cols//2, 2).transpose(1, 2)\n",
    "    P = V.unsqueeze(0) * x.unsqueeze(2)\n",
    "    Q = P * sparsity_mask.view(-1, 2).t().view(1, 2, rows, cols//2)\n",
    "    b = torch.sum(Q, dim=(1, 3))\n",
    "    torch.cuda.empty_cache()\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of an internal layer transformation at inference time: The first class is the implementation for the standard (non-compressed) weights. The second class is the implementation for the same layer when processing the compressed weights.\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ProjLayerSiluMatMul(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feature_size: int,\n",
    "        hidden_feature_size: int,\n",
    "        device: torch.device = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_feature_size = hidden_feature_size\n",
    "        self.in_feature_size = in_feature_size\n",
    "\n",
    "        self.w1 = nn.Linear(\n",
    "            in_feature_size, hidden_feature_size, bias=False, device=device\n",
    "        )\n",
    "        self.w2 = nn.Linear(\n",
    "            hidden_feature_size, in_feature_size, bias=False, device=device\n",
    "        )\n",
    "        self.w3 = nn.Linear(\n",
    "            in_feature_size, hidden_feature_size, bias=False, device=device\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        w1x = self.w1(x)\n",
    "        return self.w2(w1x * F.sigmoid(w1x) * self.w3(x))\n",
    "    \n",
    "class ProjLayerSiluMatMul_Compressed(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feature_size: int,\n",
    "        hidden_feature_size: int,\n",
    "        device: torch.device = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_feature_size = hidden_feature_size\n",
    "        self.in_feature_size = in_feature_size\n",
    "\n",
    "        self.w1 = nn.Linear(\n",
    "            in_feature_size//2, hidden_feature_size, bias=False, device=device\n",
    "        )\n",
    "        self.w2 = nn.Linear(\n",
    "            hidden_feature_size//2, in_feature_size, bias=False, device=device\n",
    "        )\n",
    "        self.w3 = nn.Linear(\n",
    "            in_feature_size//2, hidden_feature_size, bias=False, device=device\n",
    "        )\n",
    "        self.w1_mask = nn.Parameter(torch.empty((hidden_feature_size, in_feature_size//8), dtype=torch.uint8, device=device), requires_grad=False)\n",
    "        self.w2_mask = nn.Parameter(torch.empty((in_feature_size, hidden_feature_size//8), dtype=torch.uint8, device=device), requires_grad=False)\n",
    "        self.w3_mask = nn.Parameter(torch.empty((hidden_feature_size, in_feature_size//8), dtype=torch.uint8, device=device), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w1x = decompress_weights(self.w1.weight, self.w1_mask, x)\n",
    "        w3x = decompress_weights(self.w3.weight, self.w3_mask, x)\n",
    "        w2x = decompress_weights(self.w2.weight, self.w2_mask, w1x * F.sigmoid(w1x) * w3x)\n",
    "        return w2x\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
