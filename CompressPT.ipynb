{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed/miniconda3/envs/sparse/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from instruct_pipeline import InstructionTextGenerationPipeline\n",
    "os.environ[\"CUDA_MODULE_LOADING\"] = \"LAZY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model output is:  Fusion is the process of using nuclear fission to create heat and light. Nuclear fission is the process of using nuclear fission to create heat and light. Nuclear fusion is the process of using nuclear fission to create heat and light. Nuclear fusion is the process of using nuclear fission to create heat and light.\n",
      "I hope this helps!\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "pretrained_dir = 'checkpoints/sparse_dolly_12/' # 1:2 sparsity\n",
    "model_name = 'databricks/dolly-v2-12b'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_dir, torch_dtype=torch.half).to(device)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"Explain to me the difference between nuclear fission and fusion.\"\n",
    "with torch.no_grad():\n",
    "    res = generate_text(text)\n",
    "print('The model output is: ', res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_permutation(name, param):\n",
    "    W = param.clone().detach().cpu().numpy()\n",
    "    rows, cols = W.shape\n",
    "    sparsity_mask = (W != 0)\n",
    "\n",
    "    x = np.random.randn(cols, 1)\n",
    "    a = W @ x\n",
    "\n",
    "    V = W[sparsity_mask.astype(bool)].reshape(rows, cols // 2)\n",
    "    P = V[np.newaxis, ...]*x.reshape(cols// 2, 2).T[:, np.newaxis, :]\n",
    "    Q = P * sparsity_mask.reshape(-1, 2).T.reshape(2, rows, cols // 2)\n",
    "    b = np.sum(Q, axis=(0, 2))\n",
    "    assert np.allclose(a.flatten(), b) == True, f\"Failed for {name}\"\n",
    "\n",
    "def verify_dolly_sparsity(cur_model):\n",
    "    for name, param in cur_model.named_parameters():\n",
    "        if ('dense' in name or 'query_key_value' in name) and len(param.shape)>1:\n",
    "            verify_permutation(name, param)\n",
    "verify_dolly_sparsity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_permutation_torch(name, param):\n",
    "    W = param.clone().detach().to(torch.float64)\n",
    "    rows, cols = W.shape\n",
    "    sparsity_mask = (W != 0)\n",
    "\n",
    "    x = torch.randn(W.shape[1], dtype=torch.float64, device=device)\n",
    "    a = W @ x\n",
    "\n",
    "    V = W[sparsity_mask.to(bool)].reshape(rows, cols // 2)\n",
    "    P = V[np.newaxis, ...]*x.reshape(cols// 2, 2).T[:, np.newaxis, :]\n",
    "    Q = P * sparsity_mask.reshape(-1, 2).T.reshape(2, rows, cols // 2)\n",
    "    b = torch.sum(Q, dim=(0, 2))\n",
    "    assert torch.allclose(a.flatten(), b) == True, f\"Failed for {name}\"\n",
    "\n",
    "def verify_dolly_sparsity_torch(cur_model):\n",
    "    for name, param in cur_model.named_parameters():\n",
    "        if ('dense' in name or 'query_key_value' in name) and len(param.shape)>1:\n",
    "            verify_permutation_torch(name, param)\n",
    "verify_dolly_sparsity_torch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression verified!\n"
     ]
    }
   ],
   "source": [
    "def compress_weights(param):\n",
    "    W = param.clone().detach()\n",
    "    sparsity_mask = (W != 0)\n",
    "    V = W.masked_select(sparsity_mask).view(W.shape[0], -1)\n",
    "    assert V.shape[1]==W.shape[1]//2, \"Incorrect sparsity pattern\"\n",
    "    return V, sparsity_mask.bool()\n",
    "\n",
    "def decompress_weights(V, x, sparsity_mask):\n",
    "    rows, cols = sparsity_mask.shape\n",
    "    x = x.reshape(cols // 2, 2).t()\n",
    "    P = V.unsqueeze(0) * x.unsqueeze(1)\n",
    "    Q = P * sparsity_mask.reshape(-1, 2).t().reshape(2, rows, cols // 2)\n",
    "    b = torch.sum(Q, dim=(0, 2))\n",
    "    return b\n",
    "\n",
    "def verify_compression(cur_model):\n",
    "    for name, p in cur_model.named_parameters():\n",
    "        if ('dense' in name or 'query_key_value' in name) and len(p.shape) > 1:\n",
    "            V, sparsity_mask = compress_weights(p)\n",
    "            x = torch.randn(p.shape[1], dtype=torch.half, device=device)\n",
    "            b = decompress_weights(V, x, sparsity_mask)\n",
    "            a = (p.to(torch.half)@x).flatten()\n",
    "            assert torch.allclose(b, a, atol=0.0079), f\"Failed for {name}\"\n",
    "    print(\"Compression verified!\")\n",
    "verify_compression(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1910080316416714, 0.43680537607459835)\n"
     ]
    }
   ],
   "source": [
    "def compute_nonzero_mean(cur_model):\n",
    "    non_zero_sum = 0.0\n",
    "    non_zero_count = 0\n",
    "    all_non_zero_mean = []\n",
    "    for param in cur_model.parameters():\n",
    "        data = param.data\n",
    "        non_zero_values = data[data != 0]\n",
    "        non_zero_sum = torch.sum(non_zero_values)\n",
    "        non_zero_count = non_zero_values.numel()\n",
    "        non_zero_mean = non_zero_sum / non_zero_count\n",
    "        all_non_zero_mean.append(non_zero_mean.item())\n",
    "    return np.mean(all_non_zero_mean), np.std(all_non_zero_mean)\n",
    "print(compute_nonzero_mean(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189470310400 110197964800 0.581610726067613\n"
     ]
    }
   ],
   "source": [
    "def compress_weights(param):\n",
    "    W = param.clone().detach()\n",
    "    sparsity_mask = (W != 0)\n",
    "    V = W.masked_select(sparsity_mask).view(W.shape[0], -1)\n",
    "    assert V.shape[1]==W.shape[1]//2, \"Incorrect sparsity pattern\"\n",
    "    # indexes = torch.nonzero(sparsity_mask, as_tuple=False).to(torch.int32)\n",
    "    return V.to(param.dtype), sparsity_mask.to(torch.uint8)\n",
    "\n",
    "def compress_model(cur_model):\n",
    "    new_state_dict = {}\n",
    "    old_size, new_size = 0, 0\n",
    "    for name, p in cur_model.named_parameters():\n",
    "        old_size += (16 * p.nelement())\n",
    "        if ('dense' in name or 'query_key_value' in name) and len(p.shape) > 1:\n",
    "            V, sparsity_mask = compress_weights(p)\n",
    "            new_size += (16 * V.nelement() + 1 * sparsity_mask.nelement())\n",
    "            new_state_dict[name] = V.cpu()\n",
    "            new_state_dict[name + '_mask'] = sparsity_mask.cpu()\n",
    "        else:\n",
    "            new_state_dict[name] = p.cpu()\n",
    "            new_size += (16 * p.nelement())\n",
    "    return new_state_dict, old_size, new_size\n",
    "\n",
    "new_state_dict, old_size, new_size = compress_model(model)\n",
    "print(old_size, new_size, new_size/old_size)\n",
    "\n",
    "# Right now, a bool tensor uses 1 byte per element. If used 1 bit per element, we can compress the pytorch model to ~58% of its original size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
